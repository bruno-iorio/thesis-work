{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thH7IEDfz9EJ"
      },
      "source": [
        "\n",
        "# Explanation of the task:\n",
        "\n",
        "This is the classic Emotion Recognition Classification task. Given a conversation, involving 2 or more parties, for each message/utterance, we want to predict an emotion related to it.\n",
        "\n",
        "\n",
        "Consider the example:\n",
        "\n",
        "**Person A**: \"Hello! I am very happy\" (happiness)\n",
        "\n",
        "**Person B**: \"Why? I am very angry\"   (anger)\n",
        "\n",
        "## First model idea:\n",
        "- inputs: sequence of utterances, sequence of emotions.\n",
        "- For each: Linear Layers\n",
        "- Fusion model\n",
        "\n",
        "- Loss: cross-entropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahJqQ42nHjCY"
      },
      "source": [
        "# Preprocessing:\n",
        "\n",
        "Consider the each conversation as just a sequence of words:\n",
        "$$\n",
        "[[utt, utt, \\cdots], \\cdots ] \\longrightarrow [[word, word , \\cdots], \\cdots]\n",
        "$$\n",
        "\n",
        "In here, we add a separator token \"sep\". It will serve to indicate when a utteration is over, and another one starts.\n",
        "\n",
        "## Weight update:\n",
        " - Make the updatable weights constant and then update it dynamically\n",
        " - train it once and them update it dynamically in the forward pass\n",
        " - cache weight matrices and train them individually -> this does not train properly (super slow convergence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmXtaDwE1rk1",
        "outputId": "65eb36b9-80c9-480a-bc61-8e712c445b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk\n",
        "! pip install torch\n",
        "! pip install pandas\n",
        "! pip install gensim\n",
        "! pip install datasets\n",
        "! pip install matplotlib\n",
        "! pip install tqdm\n",
        "! pip install torchinfo\n",
        "\n",
        "# eventually include tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKzjVDPZz8Pw"
      },
      "outputs": [],
      "source": [
        "# ML resources:\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "#others:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "! unzip wiki-news-300d-1M.vec.zip\n",
        "! rm wiki-news-300d-1M.vec.zip"
      ],
      "metadata": {
        "id": "xzHR4n87uSwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = gensim.models.KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\",binary = False)"
      ],
      "metadata": {
        "id": "spEbLV4GunkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the embeddings and find the vocab\n",
        "import copy\n",
        "unk_token, pad_token, sep_token = '<unk>', '<pad>','<sep>'\n",
        "embedding_vectors = torch.from_numpy(encoder_model.vectors)\n",
        "pretrained_vocab = copy.deepcopy(encoder_model.index_to_key)\n",
        "pretrained_vocab[:0] = [pad_token, unk_token,sep_token]\n",
        "\n",
        "stoi = {word: i for i, word in enumerate(pretrained_vocab)}\n",
        "itos = {i: word for i, word in enumerate(pretrained_vocab)}\n",
        "\n",
        "pretrained_embeddings = torch.cat((torch.zeros(1,embedding_vectors.shape[1]),embedding_vectors))\n",
        "pretrained_embeddings = torch.cat((torch.ones(1,embedding_vectors.shape[1]),embedding_vectors))\n",
        "pretrained_embeddings = torch.cat((-torch.ones(1,embedding_vectors.shape[1]),embedding_vectors))"
      ],
      "metadata": {
        "id": "Dil129-UDo0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "max_size = 50\n",
        "## By using the template that was shared, we can process the inputs in a very similar way\n",
        "tok = TweetTokenizer()\n",
        "def tokenize_text_extend_emotions(text,emotion,stoi): ## utteration : string -> list of tokenized words : [int]\n",
        "  t1 = tok.tokenize(text)\n",
        "  text_tokenized = [stoi[word] if word in stoi else stoi['<unk>'] for word in t1]\n",
        "  return text_tokenized, [emotion]*len(t1)\n",
        "\n",
        "def concat_utt(dialog, emotions, stoi, max_size=max_size + 1): ## list of utterations : [string] -> list of list of tokenized words : [int]\n",
        "  tokenized_and_extended = [tokenize_text_extend_emotions(t,e,stoi) for t,e in zip(dialog,emotions)]\n",
        "  dialog = [i[0] for i in tokenized_and_extended]\n",
        "  emotions = [i[1] for i in tokenized_and_extended]\n",
        "  dialog_flat = []\n",
        "  emotions_extended = []\n",
        "  for i in range(len(dialog) - 1):\n",
        "    dialog[i].append(stoi[\"<sep>\"])\n",
        "    emotions[i].append(emotions[i][0])\n",
        "  for i in range(len(dialog)):\n",
        "    dialog_flat.extend(dialog[i])\n",
        "    emotions_extended.extend(emotions[i])\n",
        "  if len(dialog_flat) > max_size: ## Must cut\n",
        "    dialog_flat = dialog_flat[:max_size]\n",
        "    emotions_extended = emotions_extended[:max_size]\n",
        "  else: ## Must add padding\n",
        "    dialog_flat += [stoi[\"<pad>\"]] * (max_size - len(dialog_flat))\n",
        "    emotions_extended += [0] * (max_size - len(emotions_extended))\n",
        "  return dialog_flat,emotions_extended\n",
        "\n",
        "def adjust_emotion_labels(Y): # Necessary, because we will use 0 as padding for both the text and emotions.\n",
        "  Y_adjusted = copy.copy(Y)\n",
        "  for i in range(len(Y)):\n",
        "    for j in range(len(Y[i])):\n",
        "      Y_adjusted[i][j] = Y_adjusted[i][j] + 1\n",
        "  return Y_adjusted\n",
        "\n",
        "def preprocess_data(X,Y): ## list of lists of utterations : [[string]] -> list of lists of tokenized words : [[int]]\n",
        "  X_processed = []\n",
        "  Y_processed = []\n",
        "  Y = adjust_emotion_labels(Y) # sums 1 to all emotions (<pad_token> = 0)\n",
        "  for i in tqdm(range(len(X))):\n",
        "    X_processed.append(concat_utt(X[i],Y[i],stoi)[0])\n",
        "    Y_processed.append(concat_utt(X[i],Y[i],stoi)[1])\n",
        "  return X_processed, Y_processed\n",
        "\n",
        "def get_target(X,Y):\n",
        "  def get_other(inp):\n",
        "    return inp[:-1], inp[1:]\n",
        "  text_input = [get_other(i)[0] for i in X]\n",
        "  text_target = [get_other(i)[1] for i in X]\n",
        "  emotion_input = [get_other(i)[0] for i in Y]\n",
        "  emotion_target = [get_other(i)[1] for i in Y]\n",
        "  return text_input, text_target, emotion_input, emotion_target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Check the following example:\n",
        "dialog_example = [\"hello, I am a I robot!\",\"I am greek\"]\n",
        "emotions_example = [1,2] ## random emotions...\n",
        "\n",
        "flatten_dialog, flatten_emotions = concat_utt(dialog_example,emotions_example,stoi)\n",
        "print(f\"{dialog_example} becomes {flatten_dialog}\")\n",
        "print(f\"{emotions_example} becomes {flatten_emotions}\")\n"
      ],
      "metadata": {
        "id": "dByMWHT_Fu6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MNsIfVHjCa"
      },
      "source": [
        "## DataLoader not implemented yet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg_YM3ntHjCa"
      },
      "outputs": [],
      "source": [
        "## Modify this after changing the preprocessing.\n",
        "class DailyDialogDataset(Dataset):\n",
        "  def __init__(self, texts, emotions,target_texts,target_emotions):\n",
        "  # Dataset object for Daily Dialog dataset\n",
        "    self.texts = texts                     ## tokenized text\n",
        "    self.emotions = emotions               ## tokenized emotions\n",
        "    self.target_texts = target_texts       ## target text for loss computation\n",
        "    self.target_emotions = target_emotions ## target emotions for loss computation\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {\n",
        "                'texts': np.array(self.texts[idx]),\n",
        "             'emotions': np.array(self.emotions[idx]),\n",
        "         'target_texts': np.array(self.target_texts[idx]),\n",
        "      'target_emotions': np.array(self.target_emotions[idx])\n",
        "    }\n",
        "\n",
        "    return item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o85DNm0FXNeK"
      },
      "source": [
        "Here we will extract the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC_RQMGGXP7j"
      },
      "outputs": [],
      "source": [
        "data = load_dataset('daily_dialog')\n",
        "\n",
        "X_train = data['train']['dialog']\n",
        "Y_train = data['train']['emotion']\n",
        "\n",
        "X_test = data['test']['dialog']\n",
        "Y_test = data['test']['emotion']\n",
        "\n",
        "X_val = data['validation']['dialog']\n",
        "Y_val = data['validation']['emotion']\n",
        "mx = 0\n",
        "for i in Y_train:\n",
        "  for j in i:\n",
        "    if j > mx:\n",
        "      mx = j\n",
        "print(mx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = preprocess_data(X_train,Y_train)\n",
        "mx = 0\n",
        "for i in Y_train:\n",
        "  for j in i:\n",
        "    if j > mx:\n",
        "      mx = j\n",
        "print(mx)\n",
        "X_train,X_train_target, Y_train, Y_train_target = get_target(X_train,Y_train)\n",
        "mx = 0\n",
        "for i in Y_train:\n",
        "  for j in i:\n",
        "    if j > mx:\n",
        "      mx = j\n",
        "print(mx)\n",
        "X_test, Y_test = preprocess_data(X_test,Y_test)\n",
        "X_test,X_test_target, Y_test, Y_test_target = get_target(X_test,Y_test)\n",
        "\n",
        "X_val, Y_val = preprocess_data(X_val,Y_val)\n",
        "X_val,X_val_target, Y_val, Y_val_target = get_target(X_val,Y_val)\n",
        "\n",
        "\n",
        "\n",
        "for d, e in zip(X_train,Y_train): ## Just checking if nothing wrong happened\n",
        "  assert(len(d) == len(e))\n",
        "for d, e in zip(X_test,Y_test): ## Just checking if nothing wrong happened\n",
        "  assert(len(d) == len(e))\n",
        "for d, e in zip(X_val,Y_val): ## Just checking if nothing wrong happened\n",
        "  assert(len(d) == len(e))"
      ],
      "metadata": {
        "id": "prtVZtnj-xVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJCwMdlcRGBL"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "\n",
        "mx = 0\n",
        "for i in Y_train:\n",
        "  for j in i:\n",
        "    if j > mx:\n",
        "      mx = j\n",
        "print(mx)\n",
        "train_data = DailyDialogDataset(X_train,Y_train,X_train_target,Y_train_target)\n",
        "test_data = DailyDialogDataset(X_test_target,Y_test_target,X_test_target,Y_test_target)\n",
        "val_data = DailyDialogDataset(X_val_target,Y_val_target,X_val_target,Y_val_target)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size,shuffle = True,)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size,shuffle = True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XbNZKfP-yVrL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pyBr61UXulR"
      },
      "source": [
        "# The model:\n",
        "## archtecture\n",
        "- 2 input channels: word encoding, emotion encoding\n",
        "- dinamically updated weights: $w_1, w_2 = w1 + w2, w1$ (Not implemented yet)\n",
        "### For each channel:\n",
        "   - 3 sequential Linear layers\n",
        "- fusion linear layer through concatenation\n",
        "- 2 output channels which contain a linear layer each\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5G-cFE81iXV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class SimpleModel(nn.Module):\n",
        "  def __init__(self, emo_dim, n_emotion, n_vocab):\n",
        "    super(SimpleModel,self).__init__()\n",
        "    ## word_dim = 300\n",
        "    self.embedding_layer_text = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "    self.embedding_layer_emotion = nn.Embedding(n_emotion+1, emo_dim)\n",
        "    ## Channel for utterances/words:\n",
        "    self.Linear_utt1 = nn.Linear(300,80)\n",
        "    self.Linear_utt2 = nn.Linear(80,80)\n",
        "    self.Linear_utt3 = nn.Linear(80,80)\n",
        "    # self.Linear_utt3.requires_grad = False\n",
        "\n",
        "    ## Channel for emotions:\n",
        "    self.Linear_emo1 = nn.Linear(emo_dim,80)\n",
        "    self.Linear_emo2 = nn.Linear(80,80)\n",
        "    self.Linear_emo3 = nn.Linear(80,80)\n",
        "    # self.Linear_emo3.requires_grad = False\n",
        "\n",
        "    ## fusion by concatenation and Linear layer:\n",
        "    self.Linear_fus = nn.Linear(160,300)\n",
        "\n",
        "    ## We concatenate and do linear again (2 different concatenations)\n",
        "    self.Linear_utt_final = nn.Linear(380, n_vocab)\n",
        "    self.Linear_emo_final = nn.Linear(380, n_emotion)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, text, emotion):\n",
        "    ## first input channel:\n",
        "    #mask_text = (emb_text != 0).unsqueeze(-1).float()       ## not considered by gradient!\n",
        "    #mask_emotion = (emb_emotion != 0).unsqueeze(-1).float() ## not considered by gradient!\n",
        "\n",
        "    #x = (emb_text * mask_text).int()\n",
        "    #y = (emb_emotion * mask_emotion).int()\n",
        "\n",
        "    x = self.embedding_layer_text(text)\n",
        "    x = self.Linear_utt1(x)\n",
        "    x = self.Linear_utt2(x)\n",
        "    x = self.Linear_utt3(x)\n",
        "    print(x.size())\n",
        "    y = self.embedding_layer_emotion(emotion)\n",
        "    y = self.Linear_emo1(y)\n",
        "    y = self.Linear_emo2(y)\n",
        "    y = self.Linear_emo3(y)\n",
        "    print(y.size())\n",
        "    z = torch.cat((x,y),-1)\n",
        "    print(z.size())\n",
        "\n",
        "    z = self.Linear_fus(z)\n",
        "    print(z.size())\n",
        "\n",
        "    w = torch.cat((z,x),-1)\n",
        "    print(w.size())\n",
        "    v = torch.cat((z,y),-1)\n",
        "    print(v.size())\n",
        "    pred_token = self.Linear_utt_final(w)\n",
        "    pred_token = self.softmax(pred_token)\n",
        "    pred_emotion = self.Linear_emo_final(v)\n",
        "    pred_emotion = self.softmax(v)\n",
        "    ŕint(pred_token.size())\n",
        "    print(pred_emotion.size())\n",
        "    return pred_token, pred_emotion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwIbcOL2Q3zd"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "def activate_gpu(force_cpu=False): # check if gpu available ; code taken from template\n",
        "    device = \"cpu\"\n",
        "    if not force_cpu:\n",
        "        if torch.cuda.is_available(): # for both Nvidia and AMD GPUs\n",
        "            device = 'cuda'\n",
        "            print('DEVICE = ', torch.cuda.get_device_name(0))\n",
        "        elif torch.backends.mps.is_available(): # for mac ARM chipset\n",
        "            device = 'mps'\n",
        "            print('DEVICE = ', \"mps\" )\n",
        "        else: # for cpu only\n",
        "            device = 'cpu'\n",
        "            print('DEVICE = ', 'CPU', \"blue\")\n",
        "    return device\n",
        "\n",
        "## normal train function\n",
        "def train(model, epochs, device):\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  model.train()\n",
        "  model = model.to(device)\n",
        "  loss = 0\n",
        "  loss_to_plot = []\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for it, batch in tqdm(enumerate(train_loader),total = train_loader.__len__()):\n",
        "\n",
        "      batch = {'texts': batch['texts'].to(device),\n",
        "               'emotions': batch['emotions'].to(device),\n",
        "               'target_texts': batch['target_texts'].to(device),\n",
        "               'target_emotions': batch['target_emotions'].to(device)}\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      pt, pe  = model.forward(batch['texts'],batch['emotions'])\n",
        "      loss = loss_fn(pe,batch['target_emotions']) + loss_fn(pt, batch['target_texts'])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        "    loss_to_append = sum(losses)/len(losses)\n",
        "    loss_to_plot.append(loss_to_append)\n",
        "    print(f\"loss: \",loss_to_append)\n",
        "  return loss_to_plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMl6CqIu72Ze"
      },
      "outputs": [],
      "source": [
        "device = activate_gpu()\n",
        "emotion_dim = 30\n",
        "n_emotions = 7\n",
        "n_words = len(stoi)\n",
        "\n",
        "\n",
        "model = SimpleModel(emotion_dim,n_emotions,n_words)\n",
        "print(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_loader:\n",
        "  print(i['texts'].shape)\n",
        "  print(i['emotions'].shape)\n",
        "  print(i['target_texts'].shape)\n",
        "  print(i['target_emotions'].shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "KQGNnWm8yq0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jmIaKWuHoAa"
      },
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "losses = train(model,epochs,device)\n",
        "plt.plot(np.arange(1,epochs+1),losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHvIlbSp1hpF"
      },
      "source": [
        "# Descrition of the issues faced:\n",
        "\n",
        "It is not trivial of how to deal with the gradient flow in this case. Maybe by fixing the 2 matrix it would go better. Or just train the matrix with fixed weight. And update the weight not in the forward pass. but in the prediction. this way we can cache the weight and everytime we restart, we will be ok.\n",
        "\n",
        "I believe that, the issue of this approach specifically is updating directly the weights, and not, a hidden state, for instance.\n",
        "\n",
        "# Problem with Daily Dialog:\n",
        "By plotting the frequency of each emotion, we notice that the dataset is truly not diversified. It has essentially only 2 emotions. This is not ideal, because the models will most likely overfit into predicting those 2 emotions..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkMtblC8Je0O"
      },
      "outputs": [],
      "source": [
        "def compute_test_loss(model,loss_fn,data_loader,device):\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  accs = []\n",
        "  preds = []\n",
        "  prec = 0\n",
        "  trues = []\n",
        "  pred_words = []\n",
        "  for it,batch in tqdm(enumerate(data_loader),total=data_loader.__len__()):\n",
        "    batch = {'texts' : batch['texts'].to(device),\n",
        "             'emotions': batch['emotions'].to(device),\n",
        "             'target_texts': batch['target_texts'].to(device),\n",
        "             'target_emotions': batch['target_emotions'].to(device)}\n",
        "    pt, pe  = model.forward(batch['text'],batch['emot'])\n",
        "    loss = loss_fn(pe,batch['target_emotions'])\n",
        "    index_pred = torch.argmax(pe,1)\n",
        "    word_p = torch.argmax(pt,1)\n",
        "    correct = (batch['target_emotions'].flatten() == index_pred.flatten()).float().sum()\n",
        "    acc = correct/len(index_pred.flatten())\n",
        "    accs.append(acc.item())\n",
        "    losses.append(loss.item())\n",
        "    trues.extend(batch['target_emotions'].flatten().tolist())\n",
        "    preds.extend(index_pred.tolist())\n",
        "    pred_words.extend(word_p.tolist())\n",
        "    for i in batch['target_texts']:\n",
        "      i.item()\n",
        "  print(\"average loss: \", sum(losses)/len(losses))\n",
        "  print(\"average acc: \", sum(accs)/len(accs))\n",
        "  return trues, preds, word_p\n",
        "trues, preds, pred_words = compute_test_loss(model,nn.CrossEntropyLoss(),val_loader,device)\n",
        "names = [f'{i}' for i in range(1,8)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for i,w,q in zip(trues[:50],preds[:50],pred_words[:50]):\n",
        "#  print(i,w,itos_train1[q.item()])\n",
        "#\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, RocCurveDisplay\n",
        "print(classification_report(np.array(trues).flatten(), np.array(preds).flatten(), target_names=names))\n",
        "confusion_matrix(trues,preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z_t9a34i5uN",
        "outputId": "dfeed119-1a80-4130-b68c-a14a10177488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.96      0.72      0.82     82011\n",
            "           2       0.00      0.00      0.00      1083\n",
            "           3       0.00      0.00      0.00        41\n",
            "           4       0.16      0.17      0.16       126\n",
            "           5       0.01      0.02      0.01      6231\n",
            "           6       0.93      0.93      0.93      1085\n",
            "           7       0.81      0.27      0.40      1033\n",
            "\n",
            "    accuracy                           0.66     91610\n",
            "   macro avg       0.41      0.30      0.33     91610\n",
            "weighted avg       0.88      0.66      0.75     91610\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[59060,  9073,   402,     7, 13373,    68,    28],\n",
              "       [ 1064,     1,     5,     1,    10,     2,     0],\n",
              "       [   28,     0,     0,     0,     1,     0,    12],\n",
              "       [    7,    54,     0,    21,    24,     0,    20],\n",
              "       [  509,    20,  5588,     2,   106,     4,     2],\n",
              "       [   49,     4,     5,     1,    16,  1008,     2],\n",
              "       [  621,     4,    11,   100,    19,     3,   275]])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkAx-PPJdmWT"
      },
      "outputs": [],
      "source": [
        "def eval_sentence(model, sentence, encoded_sentence, emotions, device):\n",
        "  model.eval()\n",
        "  for i in range(len(sentence) - 1):\n",
        "    pe, _ = model.forward(encoded_sentence[i].to(device),emotions[i].to(device))\n",
        "    t = torch.max(pe,1)\n",
        "    print(f\"word:{sentence[i+1]}; predicted_emotion: {t} ; target_emotion: {emotions[i+1]}\")\n",
        "\n",
        "# eval_sentence(model,sentence,encod,emot,device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}