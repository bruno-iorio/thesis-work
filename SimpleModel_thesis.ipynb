{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thH7IEDfz9EJ"
   },
   "source": [
    "\n",
    "# Explanation of the task:\n",
    "\n",
    "This is the classic Emotion Recognition Classification task. Given a conversation, involving 2 or more parties, for each message/utterance, we want to predict an emotion related to it.\n",
    "\n",
    "\n",
    "Consider the example:\n",
    "\n",
    "**Person A**: \"Hello! I am very happy\" (happiness)\n",
    "\n",
    "**Person B**: \"Why? I am very angry\"   (anger)\n",
    "\n",
    "## First model idea:\n",
    "- inputs: sequence of utterances, sequence of emotions.\n",
    "- For each: Linear Layers\n",
    "- Fusion model\n",
    "\n",
    "- Loss: cross-entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmXtaDwE1rk1",
    "outputId": "5b7b4576-658f-4623-dc86-bf55bca79f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: torch in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pandas in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: gensim in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.1)\n",
      "Requirement already satisfied: datasets in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: torchinfo in /home/bruno/Desktop/thesis/pt_env/lib/python3.12/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install torch\n",
    "! pip install pandas\n",
    "! pip install gensim\n",
    "! pip install datasets\n",
    "! pip install matplotlib\n",
    "! pip install tqdm\n",
    "! pip install torchinfo\n",
    "# eventually include tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "yKzjVDPZz8Pw"
   },
   "outputs": [],
   "source": [
    "# ML resources:\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#others:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "punctuation = [\".\",\",\",\"?\", \"!\",\";\",\":\",\"-\",\"_\",\"(\",\")\",\"[\",\"]\",\"{\",\"}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o85DNm0FXNeK"
   },
   "source": [
    "Here we will extract the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "KC_RQMGGXP7j"
   },
   "outputs": [],
   "source": [
    "emotions_emb = {}\n",
    "\n",
    "data = load_dataset('daily_dialog')\n",
    "X_train = data['train']['dialog']\n",
    "Y_train = data['train']['emotion']\n",
    "\n",
    "X_test = data['test']['dialog']\n",
    "Y_test = data['test']['emotion']\n",
    "\n",
    "X_val = data['validation']['dialog']\n",
    "Y_val = data['validation']['emotion']\n",
    "\n",
    "bagofwords = []\n",
    "emotions = []\n",
    "\n",
    "for lx,ly in zip(X_train,Y_train):\n",
    "  assert(len(lx) == len(ly))\n",
    "for lx,ly in zip(X_val,Y_val):\n",
    "  assert(len(lx) == len(ly))\n",
    "for lx,ly in zip(X_test,Y_test):\n",
    "  assert(len(lx) == len(ly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "euWLW7lRSZhQ"
   },
   "outputs": [],
   "source": [
    "def Preprocess_Data(X_,Y_): ## We do some simple prepocessing\n",
    "  bagofwords = []\n",
    "  emotions = []\n",
    "  X = X_\n",
    "  Y = Y_\n",
    "\n",
    "  for i in range(len(X)):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for j in range(len(X[i])):\n",
    "      to_append = X[i][j].lower().split()\n",
    "      l1.extend(to_append)\n",
    "      l2.extend([Y[i][j]]*len(to_append))\n",
    "      bagofwords.append(to_append)\n",
    "    X[i] = l1\n",
    "    Y[i] = l2\n",
    "\n",
    "  for i in range(len(X)): ## remove puncuation\n",
    "    to_remove = []\n",
    "    for j in range(len(X[i])):\n",
    "      if X[i][j] in punctuation:\n",
    "        to_remove.append(j)\n",
    "    for j in to_remove[::-1]:\n",
    "      X[i].pop(j)\n",
    "      Y[i].pop(j)\n",
    "  for i in Y:\n",
    "    for j in i:\n",
    "      emotions.append(j)\n",
    "  emotions = list(set(emotions))\n",
    "\n",
    "  for lx,ly in zip(X,Y):\n",
    "    assert(len(lx) == len(ly))\n",
    "  return X_, Y_, bagofwords, emotions\n",
    "\n",
    "X_train,Y_train, bagofwords1, emotions = Preprocess_Data(X_train,Y_train)\n",
    "X_test,Y_test, bagofwords2, _ = Preprocess_Data(X_test,Y_test)\n",
    "X_val,Y_val, bagofwords3, _ = Preprocess_Data(X_val,Y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22-jdBx2t3dm",
    "outputId": "837e932e-a01b-4e34-d477-09532fc455f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  11118\n",
      "test size:  1000\n",
      "validation size:  1000\n",
      "emotions:  [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "for lx,ly in zip(X_train,Y_train):\n",
    "  assert(len(lx) == len(ly))\n",
    "for lx,ly in zip(X_val,Y_val):\n",
    "  assert(len(lx) == len(ly))\n",
    "for lx,ly in zip(X_test,Y_test):\n",
    "  assert(len(lx) == len(ly))\n",
    "\n",
    "print(\"train size: \",len(X_train))\n",
    "print(\"test size: \",len(X_test))\n",
    "print(\"validation size: \",len(X_val))\n",
    "print(\"emotions: \",emotions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "m-ZU6W8V0RZs",
    "outputId": "08d695d9-df2c-4fdc-dfe6-269aafabef31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata['train'] = data['train'].remove_columns(['id'])\\ndata['test'] = data['test'].remove_columns(['id'])\\ndata['validation'] = data['validation'].remove_columns(['id'])\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do that later on: So I can implement dataloaders and make the code cleaner\n",
    "\"\"\"\n",
    "data['train'] = data['train'].remove_columns(['id'])\n",
    "data['test'] = data['test'].remove_columns(['id'])\n",
    "data['validation'] = data['validation'].remove_columns(['id'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxOo7CsHGEIX"
   },
   "source": [
    "Now we train w2v:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBvuQEIiGHDf",
    "outputId": "a689545b-e95c-4a02-8558-af0ca4c426b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  22075\n"
     ]
    }
   ],
   "source": [
    "word_dim = 100\n",
    "w2v = Word2Vec(sentences=bagofwords1, vector_size=word_dim, min_count=1)\n",
    "\n",
    "print(\"vocabulary size: \",len(w2v.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "6S86qh2DY_vV"
   },
   "outputs": [],
   "source": [
    "if len(emotions_emb) > 0:\n",
    "    pass\n",
    "else:\n",
    "  emotions_emb = {}\n",
    "\n",
    "def encode_text(X):\n",
    "  train_X = []\n",
    "  words_list = []  \n",
    "  train_X_class = {}\n",
    "\n",
    "  n = 0\n",
    "  for dialog in X:\n",
    "    seq = []\n",
    "    for word in dialog:\n",
    "      if word not in words_list:\n",
    "        words_list.append(word)\n",
    "      try:\n",
    "        tens = torch.from_numpy(w2v.wv[word])\n",
    "        tens = tens.type(torch.float32)\n",
    "        seq.append(tens)\n",
    "      except:\n",
    "        tens = torch.rand(word_dim,dtype=torch.float32)\n",
    "        seq.append(tens)\n",
    "    train_X.append(seq)\n",
    "  stoi = {word:i for i,word in enumerate(words_list)}\n",
    "  itos = {i:word for i,word in enumerate(words_list)}\n",
    "  X_utt = []\n",
    "  for i,word in itos.items():\n",
    "    tens = torch.zeros(len(words_list))\n",
    "    tens[i] = 1\n",
    "    train_X_class[word] = tens\n",
    "  for dialog in X:\n",
    "    le = []\n",
    "    for word in dialog:\n",
    "      le.append(train_X_class[word])\n",
    "    X_utt.append(le)\n",
    "        \n",
    "\n",
    "  return train_X, stoi, itos,train_X_class,X_utt\n",
    "\n",
    "\n",
    "## We do something similar to the emotions (but we initialized them randomly)\n",
    "def encode_emotions(Y,emotions_emb):\n",
    "  train_Y = []\n",
    "  if len(emotions_emb) == 0:\n",
    "    for emotion in emotions:\n",
    "      emotions_emb[emotion] = torch.rand(word_dim,dtype=torch.float32)\n",
    "\n",
    "  for dialog in Y:\n",
    "    l = []\n",
    "    for em in dialog:\n",
    "      l.append(emotions_emb[em])\n",
    "    train_Y.append(l)\n",
    "  emotion_class = {}\n",
    "  for i in emotions:\n",
    "    ten = torch.zeros(len(emotions))\n",
    "    ten[i] = 1\n",
    "    emotion_class[i] = ten\n",
    "  Y_emo = []\n",
    "  for j in Y:\n",
    "    t = []\n",
    "    for k in j:\n",
    "      t.append(emotion_class[k])\n",
    "    Y_emo.append(t)\n",
    "  return train_Y, Y_emo, emotion_class\n",
    "\n",
    "def emotion_to_tensor(Y,emotion_class):\n",
    "  emotion_tensor = []\n",
    "  for dialog in Y:\n",
    "    l = []\n",
    "    for em in dialog:\n",
    "      l.append(emotion_class[em])\n",
    "    emotion_tensor.append(l)\n",
    "  return torch.stack(emotion_tensor)\n",
    "\n",
    "\n",
    "X_train1, stoi_train1, itos_train1, train_X_class,X_train_utt = encode_text(X_train)\n",
    "Y_train1, Y_train_emo, emotion_class = encode_emotions(Y_train,emotions_emb)\n",
    "\n",
    "X_test1, _, _, _ ,_ = encode_text(X_test)\n",
    "Y_test1, Y_test_emo, emotion_class = encode_emotions(Y_test,emotions_emb)\n",
    "\n",
    "X_val1, _, _, _,_ = encode_text(X_val)\n",
    "Y_val1, Y_val_emo, _ = encode_emotions(Y_val,emotions_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJCwMdlcRGBL",
    "outputId": "d7b42e32-0ffb-40dc-9360-9113901f6163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "for l1,l2,l3 in zip(Y_train1,X_train1,Y_train_emo):\n",
    "  assert(len(l1)==len(l2) and len(l1) == len(l3))\n",
    "print(len(Y_train_emo[1]))\n",
    "print(len(Y_train1[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pyBr61UXulR"
   },
   "source": [
    "# The model:\n",
    "## archtecture\n",
    "- 2 input channels: word encoding, emotion encoding\n",
    "- dinamically updated weights: $w_1, w_2 = w1 + w2, w1$\n",
    "### For each channel:\n",
    "   - 3 sequential Linear layers\n",
    "- fusion linear layer through concatenation\n",
    "- 2 output channels which contain a linear layer each\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "F5G-cFE81iXV"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SimpleModel(nn.Module):\n",
    "  def __init__(self, n_emb_utt, n_emb_emo, emotions_size, vocab_size,device):\n",
    "    super(SimpleModel,self).__init__()\n",
    "    ## Decide size later!\n",
    "    self.target_dev = device\n",
    "    self.current_weight = 0\n",
    "    self.decoder = nn.Linear(n_emb_utt,vocab_size)\n",
    "    ## Channel for utterances/words:\n",
    "    self.Linear_utt1 = nn.Linear(n_emb_utt,40)\n",
    "    ## Channel for emotions:\n",
    "    self.Linear_emo1 = nn.Linear(n_emb_utt,40)\n",
    "    self.Linear_emo2 = nn.Linear(40,40)\n",
    "    self.Linear_emo3 = nn.Linear(40,40)\n",
    "    ## fusion by concatenation and Linear layer:\n",
    "    self.Linear_fus = nn.Linear(80,160)\n",
    "\n",
    "    ## We concatenate and do linear again (2 different concatenations)\n",
    "    self.Linear_utt_final = nn.Linear(200, n_emb_utt)\n",
    "    self.Linear_emo_final = nn.Linear(200, emotions_size)\n",
    "\n",
    "    self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    self.Linear_hidden1 = [nn.Linear(40,40).to(device)]\n",
    "    self.Linear_hidden2 = [nn.Linear(40,40).to(device)]\n",
    "    self.Linear_hidden3 = [nn.Linear(40,40).to(device)]\n",
    "    self.Linear_hidden4 = [nn.Linear(40,40).to(device)]\n",
    "  \n",
    "  def forward_w1(self,text_emb):\n",
    "    x = self.Linear_utt1(text_emb)\n",
    "    x = self.Linear_hidden1[self.current_weight](x)\n",
    "    x = self.Linear_hidden2[self.current_weight](x)\n",
    "    return x\n",
    "\n",
    "  def forward_w2(self,emo_emb):\n",
    "    x = self.Linear_emo1(emo_emb)\n",
    "    x = self.Linear_hidden3[self.current_weight](x)\n",
    "    x = self.Linear_hidden4[self.current_weight](x)\n",
    "    return x\n",
    "\n",
    "  def forward(self, text_emb, emo_emb):\n",
    "\n",
    "    x = self.forward_w1(text_emb)\n",
    "    y = self.forward_w2(emo_emb)\n",
    "\n",
    "    z = torch.cat((x,y),-1) ## very simple\n",
    "    z = self.Linear_fus(z)\n",
    "\n",
    "    w = torch.cat((z,x),-1)\n",
    "\n",
    "    pred_token = self.Linear_utt_final(w)\n",
    "    pred_token = self.decoder(pred_token)\n",
    "    pred_token = self.softmax(pred_token)\n",
    "\n",
    "    v = torch.cat((z,y),-1)\n",
    "    v = self.Linear_emo_final(v)\n",
    "\n",
    "    pred_emotion = self.softmax(v)\n",
    "    return pred_token, pred_emotion\n",
    "\n",
    "  def predict_for_sequence(self,sequence_utt, sequence_emo):\n",
    "    pt, pe = sequence_utt[0],sequence_emo[0]\n",
    "    self.current_weight = 0\n",
    "    loss = 0\n",
    "    for j in emotions_emb.keys():\n",
    "      emotions_emb[j] = emotions_emb[j].to(self.target_dev)\n",
    "    for i in range(len(sequence_utt)):\n",
    "      pt, pe = self.forward(pt,pe)\n",
    "      pt = torch.argmax(pt).item()\n",
    "      pe = torch.argmax(pe).item()\n",
    "      if i <= len(sequence_utt)-2:\n",
    "        pt = sequence_utt[i+1]\n",
    "      else: \n",
    "        pt = itos[pt]\n",
    "      if i <= len(sequence_emo)-2:\n",
    "        pe = emotions_emb[pe] ## get embedding of emotion\n",
    "      if i < len(self.Linear_hidden1)-1:\n",
    "        self.current_weight += 1\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "          self.update_text_w()\n",
    "          self.update_emotions_w()\n",
    "          self.current_weight += 1\n",
    "    self.current_weight = 0\n",
    "    return pt, pe\n",
    "\n",
    "  def update_text_w(self):\n",
    "    with torch.no_grad():\n",
    "      new_layer1 = nn.Linear(40,40)\n",
    "      new_layer2 = nn.Linear(40,40)\n",
    "      new_weight1 = 0.2 * self.Linear_hidden1[-1].weight + 0.8 * self.Linear_hidden2[-1].weight\n",
    "      new_weight2 = self.Linear_hidden2[-1].weight.clone()\n",
    "      new_layer1.weight.copy_(new_weight1)\n",
    "      new_layer2.weight.copy_(new_weight2)\n",
    "\n",
    "      self.Linear_hidden1.append(new_layer1.to(self.target_dev))\n",
    "      self.Linear_hidden2.append(new_layer2.to(self.target_dev))\n",
    "\n",
    "  def update_emotions_w(self):\n",
    "    with torch.no_grad():\n",
    "      new_layer1 = nn.Linear(40,40)\n",
    "      new_layer2 = nn.Linear(40,40)\n",
    "      new_weight1 = 0.2 * self.Linear_hidden3[-1].weight + 0.8 * self.Linear_hidden4[-1].weight\n",
    "      new_weight2 = self.Linear_hidden3[-1].weight.clone()\n",
    "      new_layer1.weight.copy_(new_weight1)\n",
    "      new_layer2.weight.copy_(new_weight2)\n",
    "\n",
    "      self.Linear_hidden3.append(new_layer1.to(self.target_dev))\n",
    "      self.Linear_hidden4.append(new_layer2.to(self.target_dev))\n",
    "\n",
    "\n",
    "  def reset_hidden(self,device):\n",
    "    with torch.no_grad():\n",
    "      self.Linear_hidden1 = [nn.Linear(40,40).to(self.target_dev)]\n",
    "      self.Linear_hidden2 = [nn.Linear(40,40).to(self.target_dev)]\n",
    "      self.Linear_hidden3 = [nn.Linear(40,40).to(self.target_dev)]\n",
    "      self.Linear_hidden4 = [nn.Linear(40,40).to(self.target_dev)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "xwIbcOL2Q3zd"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "def activate_gpu(force_cpu=False): # check if gpu available\n",
    "    device = \"cpu\"\n",
    "    if not force_cpu:\n",
    "        if torch.cuda.is_available(): # for both Nvidia and AMD GPUs\n",
    "            device = 'cuda'\n",
    "            print('DEVICE = ', torch.cuda.get_device_name(0))\n",
    "        elif torch.backends.mps.is_available(): # for mac ARM chipset\n",
    "            device = 'mps'\n",
    "            print('DEVICE = ', \"mps\" )\n",
    "        else: # for cpu only\n",
    "            device = 'cpu'\n",
    "            print('DEVICE = ', 'CPU', \"blue\")\n",
    "    return device\n",
    "\n",
    "def cluster_training(train_X,train_Y,train_Y_emo,train_X_utt,batch_size,device):\n",
    "  i = 0\n",
    "  j = 0\n",
    "  batch_X = []\n",
    "  batch_Y = []\n",
    "  batch_emo = []\n",
    "  batch_utt = []\n",
    "  while True:\n",
    "    found = False\n",
    "    sbx = []\n",
    "    sby = []\n",
    "    sbe = []\n",
    "    sbu = []\n",
    "    lx,ly,le,lu = [], [], [], []\n",
    "    while i < len(train_X):\n",
    "      if j < len(train_X[i]) - 1:\n",
    "        if len(lx) >= batch_size:\n",
    "          sbx.append(lx.copy())\n",
    "          sby.append(ly.copy())\n",
    "          sbe.append(le.copy())\n",
    "          sbu.append(lu.copy())\n",
    "\n",
    "          lx = []\n",
    "          ly = []\n",
    "          le = []\n",
    "          lu = []\n",
    "        found = True\n",
    "        lx.append(train_X[i][j])\n",
    "        ly.append(train_Y[i][j])\n",
    "        le.append(train_Y_emo[i][j+1])\n",
    "        lu.append(train_X_utt[i][j+1])\n",
    "      i += 1\n",
    "\n",
    "    if found == False:\n",
    "      break\n",
    "    batch_X.append(sbx.copy())\n",
    "    batch_Y.append(sby.copy())\n",
    "    batch_emo.append(sbe.copy())\n",
    "    batch_utt.append(sbu.copy())\n",
    "    j += 1\n",
    "    i = 0\n",
    "  return batch_X, batch_Y, batch_emo, batch_utt\n",
    "\n",
    "def optimize_batch(model,device,bX1,bY1,bEmo1,bUtt1,optimizer):\n",
    "  loss_function = nn.CrossEntropyLoss()\n",
    "  loss = 0\n",
    "  for bX, bY, bEmo, bUtt in zip(bX1, bY1, bEmo1,bUtt1):\n",
    "    if len(bX) <= 1:\n",
    "      continue\n",
    "    utts = torch.stack(bX[:-1]).to(device)\n",
    "    emos = torch.stack(bY[:-1]).to(device)\n",
    "    next_emos = torch.stack(bEmo[1:]).to(device)\n",
    "    next_tokens = torch.stack(bUtt[1:]).to(device)\n",
    "    pred_token, pred_emotion = model.forward(utts,emos)\n",
    "    pred_token_2 = []\n",
    "    for ten in pred_token:\n",
    "      ten = torch.argmax(ten).item()\n",
    "      pred_token_2.append(train_X_class[ten].to(device))\n",
    "    pred_token_2 = torch.stack(pred_token_2).to(device)\n",
    "    loss += loss_function(pred_emotion, next_emos)\n",
    "    loss += loss_function(pred_token_2, next_tokens)\n",
    "  if type(loss) in [int, float]:\n",
    "    return loss\n",
    "  loss.backward(retain_graph=True)\n",
    "  optimizer.step()\n",
    "  return loss.item()\n",
    "\n",
    "\n",
    "def train(model, epochs, device,train_X, train_Y, train_Y_emo,train_X_utt,batch_size = 500):\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "  model.train()\n",
    "  model = model.to(device)\n",
    "  losses = []\n",
    "  batch_X, batch_Y, batch_emo,batch_utt = cluster_training(train_X,train_Y,train_Y_emo,train_X_utt,batch_size,device)\n",
    "  for i in emotion_class.keys():\n",
    "    emotion_class[i] = emotion_class[i].to(device)\n",
    "  for i in train_X_class.keys():\n",
    "    train_X_class[i].to(device)\n",
    "      \n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    x_axis = []\n",
    "    loss_batch = 0\n",
    "    model.current_weight = 0\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "    for bX1,bY1,bEmo1,bUtt1 in tqdm(zip(batch_X,batch_Y,batch_emo,batch_utt)):\n",
    "      loss_batch += optimize_batch(model,device,bX1,bY1,bEmo1,bUtt1,optimizer)\n",
    "      if epoch == 0:\n",
    "        with torch.no_grad():\n",
    "          model.update_text_w()\n",
    "          model.update_emotions_w()\n",
    "      model.current_weight += 1\n",
    "\n",
    "    loss_batch = loss_batch/(len(batch_X))\n",
    "    losses.append(loss_batch)\n",
    "    print(f\"loss: \",loss_batch)\n",
    "  return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMl6CqIu72Ze",
    "outputId": "0bd8bad3-4b36-4549-c1ad-6ff4e9c1bc72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE =  NVIDIA GeForce MX250\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = activate_gpu()\n",
    "model = SimpleModel(word_dim,word_dim,len(emotions),len(w2v.wv),device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5435b388b35d4952af52b6d8bf9bb993",
      "54f66185cb2c44e083a93e72d759657b",
      "fb4ee56dd1944cedbf3903f033206ecb",
      "9f41d052137146588a4701d8545cd37d",
      "8e37de6d07c744a4a9440dbfaa3f50be",
      "00e4411e980b468a97fed6d83224a2e3",
      "7ac33bf0518f4c8f8dc34dfca33e4060",
      "48b54b743dba4fa8bcc2527b4dae5100",
      "5801b29a33f445908ce75ab6466a2597",
      "7adc4d42374e4b41bbdb1481e48c0166",
      "1e5587a016ad4f3185725f315464ad06"
     ]
    },
    "id": "6jmIaKWuHoAa",
    "outputId": "5698fee5-01dc-4a30-ff56-7fc05d707ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9230c7c7a04961bca4110d1778d56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "6724",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train_emo\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train_utt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m),losses)\n",
      "Cell \u001b[0;32mIn[91], line 107\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, device, train_X, train_Y, train_Y_emo, train_X_utt, batch_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bX1,bY1,bEmo1,bUtt1 \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(batch_X,batch_Y,batch_emo,batch_utt)):\n\u001b[0;32m--> 107\u001b[0m   loss_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43moptimize_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbY1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbEmo1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbUtt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[91], line 74\u001b[0m, in \u001b[0;36moptimize_batch\u001b[0;34m(model, device, bX1, bY1, bEmo1, bUtt1, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ten \u001b[38;5;129;01min\u001b[39;00m pred_token:\n\u001b[1;32m     73\u001b[0m   ten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(ten)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 74\u001b[0m   pred_token_2\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_X_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43mten\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     75\u001b[0m pred_token_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(pred_token_2)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_function(pred_emotion, next_emos)\n",
      "\u001b[0;31mKeyError\u001b[0m: 6724"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "losses = train(model,epochs,device,X_train1,Y_train1,Y_train_emo,X_train_utt)\n",
    "plt.plot(np.arange(1,epochs+1),losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHvIlbSp1hpF"
   },
   "source": [
    "# Descrition of the issues faced:\n",
    "\n",
    "It is not trivial of how to deal with the gradient flow in this case. Maybe by fixing the 2 matrix it would go better. Or just train the matrix with fixed weight. And update the weight not in the forward pass. but in the prediction. this way we can cache the weight and everytime we restart, we will be ok.\n",
    "\n",
    "I believe that, the issue of this approach specifically is updating directly the weights, and not, a hidden state, for instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkMtblC8Je0O",
    "outputId": "ed719a45-b47e-4471-ce2e-b8d9fd101447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.6026026026026026\n",
      "The loss on the test set is:  1.5628195985540136\n",
      "precision:  0.6596596596596597\n",
      "The loss on the test set is:  1.5057625414969567\n"
     ]
    }
   ],
   "source": [
    "def compute_test_loss(model,X,Y,Y_emo,loss_fn,device):\n",
    "  model.eval()\n",
    "  losses = []\n",
    "  prec = 0\n",
    "  total = 0\n",
    "  for i in range(len(X)):\n",
    "    for k in range(len(X[i])):\n",
    "      X[i][k] = X[i][k].to(device)\n",
    "      Y[i][k] = Y[i][k].to(device)\n",
    "      Y_emo[i][k] = Y_emo[i][k].to(device)\n",
    "  for j in emotion_class.keys():\n",
    "    emotion_class[j] = emotion_class[j].to(device)\n",
    "  for seq_X, seq_Y, seq_emo in zip(X[:-1],Y[:-1],Y_emo[1:]):\n",
    "    loss = 0\n",
    "    pt,pe = model.predict_for_sequence(seq_X,seq_Y)\n",
    "    if pe == torch.argmax(seq_emo[-1]):\n",
    "      prec += 1\n",
    "    loss += loss_fn(emotion_class[pe],seq_emo[-1])\n",
    "    total +=1\n",
    "    losses.append(loss.item())\n",
    "  print(\"precision: \", prec/total)\n",
    "  print(\"The loss on the test set is: \", np.mean(losses))\n",
    "\n",
    "compute_test_loss(model,X_test1,Y_test1,Y_test_emo,nn.CrossEntropyLoss(),device)\n",
    "compute_test_loss(model,X_val1,Y_val1,Y_val_emo,nn.CrossEntropyLoss(),device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "wkAx-PPJdmWT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pt_env",
   "language": "python",
   "name": "pt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00e4411e980b468a97fed6d83224a2e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e5587a016ad4f3185725f315464ad06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48b54b743dba4fa8bcc2527b4dae5100": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "5435b388b35d4952af52b6d8bf9bb993": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_54f66185cb2c44e083a93e72d759657b",
       "IPY_MODEL_fb4ee56dd1944cedbf3903f033206ecb",
       "IPY_MODEL_9f41d052137146588a4701d8545cd37d"
      ],
      "layout": "IPY_MODEL_8e37de6d07c744a4a9440dbfaa3f50be"
     }
    },
    "54f66185cb2c44e083a93e72d759657b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00e4411e980b468a97fed6d83224a2e3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7ac33bf0518f4c8f8dc34dfca33e4060",
      "value": ""
     }
    },
    "5801b29a33f445908ce75ab6466a2597": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ac33bf0518f4c8f8dc34dfca33e4060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7adc4d42374e4b41bbdb1481e48c0166": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e37de6d07c744a4a9440dbfaa3f50be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f41d052137146588a4701d8545cd37d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7adc4d42374e4b41bbdb1481e48c0166",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1e5587a016ad4f3185725f315464ad06",
      "value": "â€‡7/?â€‡[00:17&lt;00:00,â€‡â€‡2.45s/it]"
     }
    },
    "fb4ee56dd1944cedbf3903f033206ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48b54b743dba4fa8bcc2527b4dae5100",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5801b29a33f445908ce75ab6466a2597",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
